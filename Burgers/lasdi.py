#!/usr/bin/env python3
#type.ignore
"""Baseline LaSDI workflow for 1D Burgers.

This script trains:
  1) an auto-encoder on full-order snapshots
  2) per-parameter latent dynamics via SINDy (strong form)
  3) simple kNN interpolation of SINDy coefficients for prediction

It assumes datasets generated by burgers_simulation.py --dataset-dir.
"""

from __future__ import annotations

import argparse
import json
import math
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Iterable, List, Sequence, Tuple

import numpy as np
from tqdm import tqdm

SCRIPT_DIR = Path(__file__).resolve().parent
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

from burgers_simulation import initial_condition  # noqa: E402

LEARNING_CASES = 5

try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
except Exception as exc:  # pragma: no cover - runtime dependency
    torch = None
    nn = None
    DataLoader = None
    TensorDataset = None
    TORCH_IMPORT_ERROR = exc
else:
    TORCH_IMPORT_ERROR = None

if TYPE_CHECKING:  # pragma: no cover - typing only
    import torch as torch_typing


@dataclass
class AEConfig:
    input_dim: int
    latent_dim: int
    hidden_sizes: Sequence[int]
    activation: str


@dataclass
class TrainConfig:
    epochs: int = 5000
    batch_size: int = 512
    lr: float = 1.0e-3
    time_stride: int = 1
    drop_endpoint: bool = False
    sindy_degree: int = 1
    sindy_threshold: float = 0.05
    sindy_max_iter: int = 10


class AutoEncoder(nn.Module):
    def __init__(self, cfg: AEConfig):
        super().__init__()
        act = activation_from_name(cfg.activation)
        enc_layers: List["torch_typing.nn.Module"] = []
        in_dim = cfg.input_dim
        for h in cfg.hidden_sizes:
            enc_layers.append(nn.Linear(in_dim, h))
            enc_layers.append(act())
            in_dim = h
        enc_layers.append(nn.Linear(in_dim, cfg.latent_dim))
        self.encoder = nn.Sequential(*enc_layers)

        dec_layers: List["torch_typing.nn.Module"] = []
        in_dim = cfg.latent_dim
        for h in reversed(cfg.hidden_sizes):
            dec_layers.append(nn.Linear(in_dim, h))
            dec_layers.append(act())
            in_dim = h
        dec_layers.append(nn.Linear(in_dim, cfg.input_dim))
        self.decoder = nn.Sequential(*dec_layers)

    def encode(self, x: torch.Tensor) -> torch.Tensor: # type: ignore
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor: # type: ignore
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> torch.Tensor: # type: ignore
        return self.decode(self.encode(x))


def activation_from_name(name: str):
    name = name.lower()
    if name == "sigmoid":
        return nn.Sigmoid
    if name == "tanh":
        return nn.Tanh
    if name == "relu":
        return nn.ReLU
    raise ValueError(f"Unknown activation: {name}")


def require_torch() -> None:
    if torch is None:
        raise RuntimeError(
            "PyTorch is required for lasdi.py. "
            "Install torch before running this script."
        ) from TORCH_IMPORT_ERROR


def load_index(dataset_dir: Path) -> List[Tuple[str, float, float]]:
    index_path = dataset_dir / "index.csv"
    if not index_path.exists():
        raise FileNotFoundError(f"index.csv not found in {dataset_dir}")
    rows = []
    with index_path.open("r", encoding="utf-8") as handle:
        header = handle.readline()
        if "filename" not in header:
            raise ValueError("index.csv header is invalid")
        for line in handle:
            filename, a_str, w_str = line.strip().split(",")
            rows.append((filename, float(a_str), float(w_str)))
    return rows


def select_corner_center_rows(
    rows: Sequence[Tuple[str, float, float]],
) -> List[Tuple[str, float, float]]:
    if not rows:
        return []

    params = np.asarray([[a, w] for _, a, w in rows], dtype=float)
    a_min, w_min = params.min(axis=0)
    a_max, w_max = params.max(axis=0)
    targets = np.asarray(
        [
            [a_min, w_min],
            [a_min, w_max],
            [a_max, w_min],
            [a_max, w_max],
            [0.5 * (a_min + a_max), 0.5 * (w_min + w_max)],
        ],
        dtype=float,
    )

    selected_idx: List[int] = []
    selected_set = set()
    for target in targets:
        dists = np.linalg.norm(params - target, axis=1)
        for idx in np.argsort(dists):
            idx_int = int(idx)
            if idx_int not in selected_set:
                selected_idx.append(idx_int)
                selected_set.add(idx_int)
                break

    # Fallback for degenerate parameter ranges where targets may collapse.
    if len(selected_idx) < LEARNING_CASES:
        for idx in range(len(rows)):
            if idx not in selected_set:
                selected_idx.append(idx)
                selected_set.add(idx)
            if len(selected_idx) == LEARNING_CASES:
                break

    return [rows[idx] for idx in selected_idx]


def downsample_time(u: np.ndarray, t: np.ndarray, stride: int) -> Tuple[np.ndarray, np.ndarray]:
    if stride <= 1:
        return u, t
    return u[::stride], t[::stride]


def load_series(
    dataset_dir: Path, filename: str, drop_endpoint: bool, time_stride: int
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    data = np.load(dataset_dir / filename)
    u = data["u"]
    t = data["t"]
    x = data["x"]
    if drop_endpoint:
        u = u[:, :-1]
        x = x[:-1]
    u, t = downsample_time(u, t, time_stride)
    return x, t, u


def collect_snapshots(
    dataset_dir: Path,
    rows: Sequence[Tuple[str, float, float]],
    time_stride: int,
    drop_endpoint: bool,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    snapshots = []
    params = []
    x_ref = None
    t_ref = None
    for filename, a, w in rows:
        x, t, u = load_series(dataset_dir, filename, drop_endpoint, time_stride)
        if x_ref is None:
            x_ref = x
            t_ref = t
        snapshots.append(u)
        params.append([a, w])
    if not snapshots:
        raise RuntimeError("No snapshots loaded")
    u_all = np.vstack(snapshots)
    return np.asarray(params), x_ref, t_ref, u_all


def normalize_data(u: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    mean = u.mean(axis=0)
    std = u.std(axis=0)
    std[std < 1.0e-12] = 1.0
    u_norm = (u - mean) / std
    return u_norm, mean, std


def train_autoencoder(
    u_norm: np.ndarray,
    ae_cfg: AEConfig,
    train_cfg: TrainConfig,
    device: torch.device, # type: ignore
) -> AutoEncoder:
    require_torch()
    model = AutoEncoder(ae_cfg).to(device)
    dataset = TensorDataset(torch.from_numpy(u_norm).float())
    loader = DataLoader(dataset, batch_size=train_cfg.batch_size, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=train_cfg.lr)
    loss_fn = nn.MSELoss()

    model.train()
    pbar = tqdm(range(train_cfg.epochs), desc="Training AutoEncoder")
    for epoch in pbar:
        running = 0.0
        for (batch,) in loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            recon = model(batch)
            loss = loss_fn(recon, batch)
            loss.backward()
            optimizer.step()
            running += loss.item() * batch.size(0)
        avg = running / len(dataset)
        pbar.set_postfix({"loss": f"{avg:.6e}"})
    pbar.close()
    return model


def encode_series(
    model: AutoEncoder,
    u: np.ndarray,
    mean: np.ndarray,
    std: np.ndarray,
    device: torch.device, # type: ignore
    batch_size: int,
) -> np.ndarray:
    require_torch()
    model.eval()
    u_norm = (u - mean) / std
    z_list = []
    with torch.no_grad():
        for i in range(0, u_norm.shape[0], batch_size):
            batch = torch.from_numpy(u_norm[i : i + batch_size]).float().to(device)
            z = model.encode(batch).cpu().numpy()
            z_list.append(z)
    return np.vstack(z_list)


def build_terms(latent_dim: int, degree: int, include_constant: bool = True):
    terms = []
    if include_constant:
        terms.append(("1",))
    if degree >= 1:
        for i in range(latent_dim):
            terms.append(("z", i))
    if degree >= 2:
        for i in range(latent_dim):
            for j in range(i, latent_dim):
                terms.append(("zz", i, j))
    return terms


def term_names(terms) -> List[str]:
    names = []
    for term in terms:
        if term[0] == "1":
            names.append("1")
        elif term[0] == "z":
            names.append(f"z{term[1]}")
        elif term[0] == "zz":
            names.append(f"z{term[1]}*z{term[2]}")
    return names


def evaluate_terms(z: np.ndarray, terms) -> np.ndarray:
    values = np.empty(len(terms), dtype=z.dtype)
    for k, term in enumerate(terms):
        if term[0] == "1":
            values[k] = 1.0
        elif term[0] == "z":
            values[k] = z[term[1]]
        elif term[0] == "zz":
            values[k] = z[term[1]] * z[term[2]]
    return values


def build_library(z: np.ndarray, terms) -> np.ndarray:
    theta = np.empty((z.shape[0], len(terms)), dtype=z.dtype)
    for i in range(z.shape[0]):
        theta[i] = evaluate_terms(z[i], terms)
    return theta


def time_derivative(z: np.ndarray, dt: float) -> np.ndarray:
    dzdt = np.empty_like(z)
    dzdt[1:-1] = (z[2:] - z[:-2]) / (2.0 * dt)
    dzdt[0] = (z[1] - z[0]) / dt
    dzdt[-1] = (z[-1] - z[-2]) / dt
    return dzdt


def stlsq(theta: np.ndarray, dzdt: np.ndarray, threshold: float, max_iter: int) -> np.ndarray:
    xi = np.linalg.lstsq(theta, dzdt, rcond=None)[0]
    for _ in range(max_iter):
        small = np.abs(xi) < threshold
        xi[small] = 0.0
        for k in range(xi.shape[1]):
            big = ~small[:, k]
            if not np.any(big):
                continue
            xi[big, k] = np.linalg.lstsq(theta[:, big], dzdt[:, k], rcond=None)[0]
    return xi


def fit_sindy(z: np.ndarray, dt: float, degree: int, threshold: float, max_iter: int):
    terms = build_terms(z.shape[1], degree)
    theta = build_library(z, terms)
    dzdt = time_derivative(z, dt)
    xi = stlsq(theta, dzdt, threshold, max_iter)
    return xi, terms


def interpolate_coeffs(
    params: np.ndarray,
    coeffs: np.ndarray,
    target: np.ndarray,
    k: int,
) -> np.ndarray:
    if k <= 1 or params.shape[0] == 1:
        idx = int(np.argmin(np.linalg.norm(params - target, axis=1)))
        return coeffs[idx]
    dists = np.linalg.norm(params - target, axis=1)
    idx = np.argsort(dists)[:k]
    weights = 1.0 / (dists[idx] + 1.0e-12)
    weights /= weights.sum()
    return np.tensordot(weights, coeffs[idx], axes=(0, 0))


def integrate_latent(z0: np.ndarray, coeffs: np.ndarray, terms, dt: float, steps: int) -> np.ndarray:
    z = np.empty((steps + 1, z0.size), dtype=z0.dtype)
    z[0] = z0

    def rhs(z_state: np.ndarray) -> np.ndarray:
        theta = evaluate_terms(z_state, terms)
        return theta @ coeffs

    for n in range(steps):
        k1 = rhs(z[n])
        k2 = rhs(z[n] + 0.5 * dt * k1)
        k3 = rhs(z[n] + 0.5 * dt * k2)
        k4 = rhs(z[n] + dt * k3)
        z[n + 1] = z[n] + (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)
    return z


def decode_series(
    model: AutoEncoder,
    z: np.ndarray,
    mean: np.ndarray,
    std: np.ndarray,
    device: torch.device, # type: ignore
    batch_size: int,
) -> np.ndarray:
    require_torch()
    model.eval()
    u_list = []
    with torch.no_grad():
        for i in range(0, z.shape[0], batch_size):
            batch = torch.from_numpy(z[i : i + batch_size]).float().to(device)
            u_norm = model.decode(batch).cpu().numpy()
            u_list.append(u_norm)
    u_norm = np.vstack(u_list)
    return u_norm * std + mean


def save_model(
    model_dir: Path,
    model: AutoEncoder,
    ae_cfg: AEConfig,
    train_cfg: TrainConfig,
    mean: np.ndarray,
    std: np.ndarray,
    params: np.ndarray,
    coeffs: np.ndarray,
    terms,
    x: np.ndarray,
    t: np.ndarray,
):
    model_dir.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), model_dir / "ae.pt")
    np.savez(model_dir / "normalization.npz", mean=mean, std=std)
    np.save(model_dir / "params.npy", params)
    np.save(model_dir / "sindy_coeffs.npy", coeffs)
    np.savez(model_dir / "grid.npz", x=x, t=t)
    with (model_dir / "sindy_terms.json").open("w", encoding="utf-8") as handle:
        json.dump(term_names(terms), handle, indent=2)

    config = {
        "input_dim": int(ae_cfg.input_dim),
        "latent_dim": int(ae_cfg.latent_dim),
        "hidden_sizes": list(ae_cfg.hidden_sizes),
        "activation": ae_cfg.activation,
        "epochs": int(train_cfg.epochs),
        "batch_size": int(train_cfg.batch_size),
        "lr": float(train_cfg.lr),
        "time_stride": int(train_cfg.time_stride),
        "drop_endpoint": bool(train_cfg.drop_endpoint),
        "sindy_degree": int(train_cfg.sindy_degree),
        "sindy_threshold": float(train_cfg.sindy_threshold),
        "sindy_max_iter": int(train_cfg.sindy_max_iter),
    }
    with (model_dir / "config.json").open("w", encoding="utf-8") as handle:
        json.dump(config, handle, indent=2)


def load_model(model_dir: Path, device: torch.device): # type: ignore
    require_torch()
    with (model_dir / "config.json").open("r", encoding="utf-8") as handle:
        cfg = json.load(handle)
    ae_cfg = AEConfig(
        input_dim=cfg["input_dim"],
        latent_dim=cfg["latent_dim"],
        hidden_sizes=cfg["hidden_sizes"],
        activation=cfg["activation"],
    )
    model = AutoEncoder(ae_cfg).to(device)
    model.load_state_dict(torch.load(model_dir / "ae.pt", map_location=device))
    model.eval()

    norm = np.load(model_dir / "normalization.npz")
    mean = norm["mean"]
    std = norm["std"]
    params = np.load(model_dir / "params.npy")
    coeffs = np.load(model_dir / "sindy_coeffs.npy")
    grid = np.load(model_dir / "grid.npz")
    x = grid["x"]
    t = grid["t"]
    with (model_dir / "sindy_terms.json").open("r", encoding="utf-8") as handle:
        term_labels = json.load(handle)

    terms = build_terms(ae_cfg.latent_dim, cfg["sindy_degree"])
    if term_names(terms) != term_labels:
        raise RuntimeError("Stored SINDy terms do not match generated terms")

    return model, mean, std, params, coeffs, terms, x, t, cfg


def train_pipeline(args: argparse.Namespace) -> None:
    require_torch()
    dataset_dir = Path(args.dataset_dir)
    model_dir = Path(args.model_dir)
    rows = load_index(dataset_dir)
    learning_rows = select_corner_center_rows(rows)
    if not learning_rows:
        raise RuntimeError("Dataset index is empty")
    print(f"Using {len(learning_rows)} training cases (4 corners + center)")
    for _, a, w in learning_rows:
        print(f"  selected (a={a:.6f}, w={w:.6f})")

    if args.time_stride < 1:
        raise ValueError("time_stride must be >= 1")

    params, x, t, u_snapshots = collect_snapshots(
        dataset_dir,
        learning_rows,
        args.time_stride,
        args.drop_endpoint,
    )
    if t.size < 2:
        raise RuntimeError("Time grid must contain at least two points")
    dt = t[1] - t[0]

    u_norm, mean, std = normalize_data(u_snapshots)

    ae_cfg = AEConfig(
        input_dim=u_norm.shape[1],
        latent_dim=args.latent_dim,
        hidden_sizes=tuple(args.hidden_sizes),
        activation=args.activation,
    )
    train_cfg = TrainConfig(
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        time_stride=args.time_stride,
        drop_endpoint=args.drop_endpoint,
        sindy_degree=args.sindy_degree,
        sindy_threshold=args.sindy_threshold,
        sindy_max_iter=args.sindy_max_iter,
    )

    device = torch.device("cpu" if args.cpu or not torch.cuda.is_available() else "cuda")
    model = train_autoencoder(u_norm, ae_cfg, train_cfg, device)
    print("device:", device)

    coeffs_list = []
    params_used = []
    for filename, a, w in learning_rows:
        _, t_series, u_series = load_series(dataset_dir, filename, args.drop_endpoint, args.time_stride)
        z = encode_series(model, u_series, mean, std, device, args.batch_size)
        xi, terms = fit_sindy(
            z,
            dt,
            args.sindy_degree,
            args.sindy_threshold,
            args.sindy_max_iter,
        )
        coeffs_list.append(xi)
        params_used.append([a, w])
        print(f"SINDy fit for a={a:.3f}, w={w:.3f} terms={xi.shape[0]}")

    coeffs = np.stack(coeffs_list, axis=0)
    params_used = np.asarray(params_used)

    save_model(model_dir, model, ae_cfg, train_cfg, mean, std, params_used, coeffs, terms, x, t)
    print(f"Saved model to {model_dir}")


def predict_pipeline(args: argparse.Namespace) -> None:
    require_torch()
    model_dir = Path(args.model_dir)
    device = torch.device("cpu" if args.cpu or not torch.cuda.is_available() else "cuda")
    model, mean, std, params, coeffs, terms, x, t, cfg = load_model(model_dir, device)

    a = float(args.a)
    w = float(args.w)
    target = np.array([a, w], dtype=float)

    dt = t[1] - t[0]
    steps = t.size - 1
    coeffs_interp = interpolate_coeffs(params, coeffs, target, args.knn)

    u0 = initial_condition(x, a, w)
    if not cfg["drop_endpoint"]:
        u0[-1] = u0[0]
    z0 = encode_series(model, u0[None, :], mean, std, device, args.batch_size)[0]

    z = integrate_latent(z0, coeffs_interp, terms, dt, steps)
    u_pred = decode_series(model, z, mean, std, device, args.batch_size)

    if not cfg["drop_endpoint"]:
        u_pred[:, -1] = u_pred[:, 0]

    out = Path(args.output)
    np.savez(out, a=a, w=w, x=x, t=t, u=u_pred)
    print(f"Saved prediction to {out}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Baseline LaSDI for Burgers.")
    sub = parser.add_subparsers(dest="command", required=True)

    train = sub.add_parser("train", help="Train AE + SINDy on dataset")
    train.add_argument("--dataset-dir", required=True, help="Dataset directory")
    train.add_argument("--model-dir", required=True, help="Output model directory")
    train.add_argument("--latent-dim", type=int, default=5, help="Latent dimension")
    train.add_argument(
        "--hidden-sizes",
        type=lambda s: [int(v) for v in s.split(",") if v],
        default=[100],
        help="Comma-separated hidden sizes (e.g., 100,100)",
    )
    train.add_argument("--activation", type=str, default="sigmoid", help="Activation")
    train.add_argument("--epochs", type=int, default=5000, help="Training epochs")
    train.add_argument("--batch-size", type=int, default=512, help="Batch size")
    train.add_argument("--lr", type=float, default=1.0e-3, help="Learning rate")
    train.add_argument("--time-stride", type=int, default=1, help="Time stride")
    train.add_argument(
        "--drop-endpoint",
        action="store_true",
        help="Drop periodic endpoint before training",
    )
    train.add_argument("--sindy-degree", type=int, default=1, help="SINDy degree")
    train.add_argument("--sindy-threshold", type=float, default=0.05, help="STLSQ threshold")
    train.add_argument("--sindy-max-iter", type=int, default=10, help="STLSQ iterations")
    train.add_argument("--cpu", action="store_true", help="Force CPU")

    predict = sub.add_parser("predict", help="Predict for a new (a, w)")
    predict.add_argument("--model-dir", required=True, help="Model directory")
    predict.add_argument("--a", type=float, required=True, help="Amplitude parameter a")
    predict.add_argument("--w", type=float, required=True, help="Width parameter w")
    predict.add_argument("--knn", type=int, default=1, help="kNN neighbors for coeffs")
    predict.add_argument(
        "--output",
        type=str,
        default="prediction.npz",
        help="Output .npz file",
    )
    predict.add_argument("--batch-size", type=int, default=512, help="Batch size")
    predict.add_argument("--cpu", action="store_true", help="Force CPU")

    return parser.parse_args()


def main() -> None:
    args = parse_args()
    if args.command == "train":
        train_pipeline(args)
    elif args.command == "predict":
        predict_pipeline(args)


if __name__ == "__main__":
    main()
